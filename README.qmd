---
title: "Activity 12: Statistical reasoning 4: prediction and evaluation"
subtitle: Feb. 18th, 2026, Calvin Munson
format: gfm
execute:
  warning: false
editor: source
---

![XKCD-weather_balloons](photos/XKCD-weather_balloons.png)

Welcome! This is the fourth statistical reasoning activity. The goals of this activity are to understand how to evaluate and compare the predictive accuracy of multiple models. Specifically, you will:

-   Run and interpret multiple models on the same dataset and evaluate them to see which is best supported using WAIC and PSIS.

------------------------------------------------------------------------

You will submit one output for this activity:

1.  A **PDF** of a rendered Quarto document with all of your R code. Please create a new Quarto document (e.g. don't use this `README.qmd`) and include all of the code that appears in this document, your own code, and **answers to all of the questions** in the "Q#" sections. Submit this PDF through Gradescope.

A reminder: **Please label the code** in your final submission in two ways:

1.  denote your answers to each question using headers that correspond to the question you're answering, and
2.  thoroughly "comment" your code: remember, this means annotating your code directly by typing descriptions of what each line does after a `#`. This will help future you!

------------------------------------------------------------------------

# 1. Practice with model comparison

------------------------------------------------------------------------

By making models, we are trying to approximate the processes that affect things in a system. It's important to know how well our models are actually aligning with reality though. If we are not careful, we may "overfit" our models. *Overfitting* is when our model fits the data too closely, usually because we added too many parameters, which explains the data well but makes the model very bad at predicting future data (for instance, the final panel of the XKCD comic below).

![XKCD_curve_fitting](photos/XKCD_curve_fitting.png)

It is common in the field of ecology to have multiple candidate models of how a system works. How do we know which is "best" at making predictions? In this activity we will learn two metrics that can help: the Watanabe–Akaike information criterion (**WAIC**) and Pareto Smoothed Importance Sampling (**PSIS**).

Both metrics tell us how well the model will predict data it wasn't trained on, which is important for thinking about how well the model might predict new data that we as scientists have not encountered yet.

------------------------------------------------------------------------

Let's start by reading in the relevant packages

```{r}
library(brms) # for statistics
library(tidyverse) # for data wrangling
library(lterdatasampler)
```

We are going to work with the fiddler crab and latitude data again:

```{r}
pie_crab <- lterdatasampler::pie_crab
```

------------------------------------------------------------------------

In this section, we will run and interpret three multiple regressions to try and understand what influences crab body width in mm (`size`). These are data from crabs (~30 per site) collected from sites from Florida to Massachusetts. Let's remind ourselves of the columns in the crab data:

```{r}
colnames(pie_crab)
```

We have multiple temperature variables that may be relevant to crab body size here, all measured in degrees Celsius. Mean annual air and water temperature data (`air_temp`, `water_temp`), plus the standard deviations of each (`air_temp_sd` and `water_temp_sd`, representing variability in temperature and perhaps seasonality).

------------------------------------------------------------------------

## 1.1 Create hypotheses for how each variable may affect crab size

Create four separate hypotheses describing how each predictor would be associated with larger or smaller crabs. Why? Please write 2-3 sentences for each predictor.

------------------------------------------------------------------------

### Q1.1a How might *mean* annual *water* temperature affect crab size?

------------------------------------------------------------------------

### Q1.1b How might *mean* annual *air* temperature affect crab size?

------------------------------------------------------------------------

### Q1.1c How might the *sd* (variability) of *water* temperature affect crab size?

------------------------------------------------------------------------

### Q1.1d How might the *sd* (variability) of *air* temperature affect crab size?

------------------------------------------------------------------------

## 1.2 Run, assess, and interpret three multiple regressions with latitude and *mean* temperatures

Let's run three regressions and compare their results. We will start by looking at how body size varies with latitude plus each of the mean temperature values separately, then together. Since these are intertidal estuarine sites, crabs are exposed to water for part of the day and air for another part; air and/or water temperatures may be important. The models will be:

-   size ~ latitude + water_temp
-   size ~ latitude + air_temp
-   size ~ latitude + water_temp + air_temp

------------------------------------------------------------------------

### size ~ latitude + mean water temp

```{r}
# latitude and water model
m.crab.lat.water <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + water_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "output/m.crab.lat.water")
```

Now look at the output:

```{r}
summary(m.crab.lat.water)
```

```{r}
plot(m.crab.lat.water)
```

------------------------------------------------------------------------

#### Q1.2a Assess the output

Assess whether the model ran correctly by looking at R hat, the chains, and the posterior distributions using the plot() and summary() functions. Describe your thought process about whether the model ran correctly in 1-2 sentences.


------------------------------------------------------------------------

#### Q1.2b Interpret the output

Interpret your model by answering:

1.  What are the effects of your predictors? Remember to describe the effect using the units to make it biologically meaningful.
2.  Are the effects reasonably different from zero? How do you know?


------------------------------------------------------------------------

### size ~ latitude + mean air temp

```{r}
# latitude and air model
m.crab.lat.air <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + air_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "output/m.crab.lat.air")
```

```{r}
summary(m.crab.lat.air)
plot(m.crab.lat.air)
```

------------------------------------------------------------------------

#### Q1.3a Assess the output

Assess whether the model ran correctly by looking at R hat, the chains, and the posterior distributions using the plot() and summary() functions. Describe your thought process about whether the model ran correctly in 1-2 sentences.



------------------------------------------------------------------------

#### Q1.3b Interpret the output

Interpret your model by answering:

1.  What are the effects of your predictors? Remember to describe the effect using the units to make it biologically meaningful.
2.  Are the effects reasonably different from zero? How do you know?


------------------------------------------------------------------------

### size ~ latitude + mean water + mean air temp

```{r}
# latitude and air model
m.crab.lat.air.water <- 
  brm(data = pie_crab, # Give the model the pie_crab data
      # Choose a gaussian (normal) distribution
      family = gaussian,
      # Specify the model here. 
      size ~ latitude + air_temp + water_temp,
      # Here's where you specify parameters for executing the Markov chains
      # We're using similar to the defaults, except we set cores to 4 so the analysis runs faster than the default of 1
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      # Setting the "seed" determines which random numbers will get sampled.
      # In this case, it makes the randomness of the Markov chain runs reproducible 
      # (so that both of us get the exact same results when running the model)
      seed = 4,
      # Save the fitted model object as output - helpful for reloading in the output later
      file = "output/m.crab.lat.air.water")
```

```{r}
summary(m.crab.lat.air.water)
plot(m.crab.lat.air.water)
```

------------------------------------------------------------------------

#### Q1.4a Assess the output

Assess whether the model ran correctly by looking at R hat, the chains, and the posterior distributions using the plot() and summary() functions. Describe your thought process about whether the model ran correctly in 1-2 sentences.



------------------------------------------------------------------------

#### Q1.4b Interpret the output

Interpret your model by answering:

1.  What are the effects of your predictors? Remember to describe the effect using the units to make it biologically meaningful.
2.  Are the effects reasonably different from zero? How do you know?


------------------------------------------------------------------------

#### Q1.5 How do the models differ in their estimates?

In 2-4 sentences, compare the three models' estimates of the effect of latitude, water temp, and air temp; did estimates change across different models? Stay the same? Change in whether or not they are different from zero?


------------------------------------------------------------------------

#### Q1.6 Why do you think a variable's sign changed?

You should have noticed the change in sign for a variable. In 1-2 sentences, and in the context of your knowledge about causal inference from DAGs from last week, describe why you think the variable may have changed signs (hint: remember pipes?).



------------------------------------------------------------------------

## 1.3 Compare models using WAIC and PSIS

We just compared the models in terms of what values they provided for the estimates of the effects of `latitude`, `water_temp`, and `air_temp`. Now we are going to compare models using the Pareto Smoothed Importance Sampling (**PSIS**) and Watanabe–Akaike information criterion (**WAIC**). Remember, both of these metrics will tell us about a model's out of sample predictive skill. Lower values = better! A major reason we due this is to avoid *overfitting*, where more complex models with lots of parameters are un-generalizable to out of sample data.

First, let's look at the PSIS output from the three models. Remember, lower values are better, and more complicated models (models with more parameters) will be "punished", since more parameters risks overfitting. The PSIS values will be in the `Estimate` column in the third row.

```{r}
# Look at "leave one out" results for all three models
# size ~ lat + mean water
loo(m.crab.lat.water)
# size ~ lat + mean air
loo(m.crab.lat.air)
# size ~ lat + mean water + mean air
loo(m.crab.lat.air.water)
```

The first thing to look for to assess the leave one out method is the Pareto k estimate. For us, it gives the helpful message: `All Pareto k estimates are good (k < 0.7)`.

The last row in each table's output is our PSIS value. Remember, lower = better. As we can see, the `size ~ latitude + mean water + mean air` model had the lowers PSIS value, despite having the most parameters. This indicates that the extra parameter made up for the punishment by adding much more predictive power.

------------------------------------------------------------------------

Now we do the same for WAIC:

```{r}
# Look at "leave one out" results for all three models
# size ~ lat + mean water
waic(m.crab.lat.water)
# size ~ lat + mean air
waic(m.crab.lat.air)
# size ~ lat + mean water + mean air
waic(m.crab.lat.air.water)

```

### Q1.7 Which model has the "best" WAIC value?

Remember, lower is better!


------------------------------------------------------------------------

Importantly, we want both the PSIS results and the WAIC results to align. In this case, they do, which is a good sign for our models.

------------------------------------------------------------------------

## 1.4 Look at uncertainty around model predictions

Here we will look at some of the ways we can look at the uncertainty around model predictions form the "model evaluation" lecture using the most complex model with `size ~ latitude + mean water + mean air`.

The `predict_response(interval = "prediction")` function plots the 95% prediction intervals separately for each predictor, displaying uncertainty around where the data may lay around the model.

```{r}
preds <- ggeffects::predict_response(m.crab.lat.air.water, 
                            interval = "prediction")
plot(preds)
```

There are a few posterior predictive check plots we can look at. For instance, `pp_check(type = "dens_overlay")` shows the probability density of the observed data in a heavy line. The thin blue lines show the range of probability densities that are expected if you simulate from the fitted model's range of estimated posteriors. We want the thin blue lines to align pretty well with the heavy line.

```{r}
pp_check(m.crab.lat.air.water, type = "dens_overlay")
```

`pp_check(type = "scatter_avg")` shows the observed values on the y axis and the average of the predicted values on the x axis using a scatterplot. Having all of the points fall along the 1:1 line would indicate good fit. Points that fall outside that line can help us understand whether there are missing predictors.

```{r}
pp_check(m.crab.lat.air.water, type = "scatter_avg")
```

Here, it seems that there is a lot of variation at each site: remember, each site (which has one value of latitude) has ~30 crabs. The model line goes through approximately the middle of each cloud of points, which is good, but there is still a lot of unexplained variation at seemingly the site level. This may mean that there are site-specific variables that are causing variation in crab size that are not captured in this model.

------------------------------------------------------------------------

## 2. Repeat with the sd of water and air temp instead of mean temp

------------------------------------------------------------------------

Now it's your turn! In this section, repeat what we just did but with the standard deviation (sd) of water and air temperature instead of the mean air and water temperature.

The three models should be:

-   size ~ latitude + water_temp_sd
-   size ~ latitude + air_temp_sd
-   size ~ latitude + water_temp_sd + air_temp_sd

------------------------------------------------------------------------

### Q2.1 Run all three models

Run and store all three models. Remember to change the name of 1) the data that the model output is stored as and 2) the output file name


#### size ~ latitude + water temp sd

------

#### size ~ latitude + air temp sd

------------------------------------------------------------------------

#### size ~ latitude + water temp sd + air temp sd


------------------------------------------------------------------------

### Q2.2 Assess all three models

Assess whether each model ran correctly by looking at R hat, the chains, and the posterior distributions using the plot() and summary() functions. Describe your thought process about whether the model ran correctly in 1-2 sentences per model.


------------------------------------------------------------------------

### Q2.3 Interpret all three models

Interpret all three models by answering:

1.  What are the effects of your predictors? Remember to describe the effect using the units to make it biologically meaningful.
2.  Are the effects reasonably different from zero? How do you know?

Please write 2-3 sentences for each model



------------------------------------------------------------------------

### Q2.4 How do the models differ in their parameter estimates?

In 2-4 sentences, compare the three models' estimates of the effect of latitude, water temp sd, and air temp sd; did estimates change across different models? Stay the same? Change in whether or not they are different from zero?


------------------------------------------------------------------------

### Q2.5 Calculate and compare PSIS and AIC values for each model

Calculate and compare the PSIS and AIC values for each model and answer:

1.  Are the Pareto k estimates good?
2.  Which model has the lowest PSIS?
3.  Which model has the lowest AIC?
4.  Do PSIS and AIC values agree on which model has the best out of sample prediction?



------------------------------------------------------------------------

### Render to PDF

When you have finished, remember to pull, stage, commit, and push with GitHub:

-   Pull to check for updates to the remote branch
-   Stage your edits (after saving your document!) by checking the documents you'd like to push
-   Commit your changes with a commit message
-   Push your changes to the remote branch

Then submit the well-labeled PDF on Gradescope. Thanks!

![StrangePlanet_NathanPyle_weatherForecasting](photos/StrangePlanet_weatherForecasting.jpeg)
